{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ Stable Diffusion XL (SDXL) Image Generator\n",
    "\n",
    "**Colab-Ready, Version-Locked (May 2025, tested).**\n",
    "\n",
    "This notebook uses SDXL 1.0, LoRA/Refiner support, with Gradio 4.44.x UI.\n",
    "\n",
    "**Features:**\n",
    "- High-res 1024x1024 SDXL images\n",
    "- LoRA support (with HF model path)\n",
    "- SDXL Refiner support\n",
    "- Negative prompt\n",
    "- Quantized loading for Colab GPUs\n",
    "- Full dependency sanity-check (no conflicting installs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 1. Clean and Install Compatible Packages (Colab Only!)\n",
    "# ----------------------------------------------------------\n",
    "!pip cache purge\n",
    "!pip uninstall -y diffusers transformers huggingface_hub gradio fastapi pydantic websockets sentence-transformers peft xformers\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Install only compatible, tested versions\n",
    "!pip install -q \\\n",
    "  torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \\\n",
    "  diffusers==0.27.2 \\\n",
    "  transformers==4.39.3 \\\n",
    "  huggingface_hub==0.22.2 \\\n",
    "  accelerate==0.28.0 \\\n",
    "  safetensors==0.4.2 \\\n",
    "  xformers==0.0.28.post1 \\\n",
    "  gradio==4.44.1 \\\n",
    "  fastapi==0.110.2 \\\n",
    "  pydantic==2.7.1 \\\n",
    "  bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "import gradio as gr\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models (Base & Refiner, Quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For memory efficiency, always use torch.float16 on Colab\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "def load_base_model():\n",
    "    global pipe\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "        device_map=\"balanced\",\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "def load_refiner_model():\n",
    "    global refiner\n",
    "    refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "        device_map=\"balanced\",\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "    refiner.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "pipe = None\n",
    "refiner = None\n",
    "load_base_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(prompt, negative_prompt=\"\", num_steps=30, guidance_scale=7.5,\n",
    "                  use_refiner=False, refiner_steps=10, lora_model=\"\", lora_scale=0.5,\n",
    "                  advanced_mode=False):\n",
    "    global pipe, refiner\n",
    "    try:\n",
    "        cross_attention_kwargs = {}\n",
    "        if advanced_mode and lora_model.strip():\n",
    "            try:\n",
    "                pipe.unet.load_attn_procs(lora_model)\n",
    "                cross_attention_kwargs = {\"scale\": lora_scale}\n",
    "            except Exception as e:\n",
    "                print(f\"LoRA loading error: {e}\")\n",
    "                return Image.new(\"RGB\", (512, 512), color=\"black\")\n",
    "        # Generate base image\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=num_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            cross_attention_kwargs=cross_attention_kwargs\n",
    "        ).images[0]\n",
    "        # Optionally refine\n",
    "        if advanced_mode and use_refiner:\n",
    "            if refiner is None:\n",
    "                load_refiner_model()\n",
    "            image = refiner(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                num_inference_steps=refiner_steps,\n",
    "                image=image\n",
    "            ).images[0]\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {e}\")\n",
    "        return Image.new(\"RGB\", (512, 512), color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio UI (Colab-Ready, Auto-Port, Debug Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ðŸŽ¨ Stable Diffusion XL 1.0 (May 2025)\\nGenerate high-quality images from text.\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            prompt = gr.Textbox(label=\"Prompt\", placeholder=\"A haunted forest at dusk\", lines=2)\n",
    "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", placeholder=\"blurry, low quality\", lines=1)\n",
    "            num_steps = gr.Slider(1, 100, value=30, label=\"Inference Steps\")\n",
    "            guidance_scale = gr.Slider(1.0, 20.0, value=7.5, label=\"Guidance Scale\")\n",
    "            advanced = gr.Checkbox(label=\"Advanced Mode (LoRA/Refiner)\", value=False)\n",
    "            use_refiner = gr.Checkbox(label=\"Use Refiner\", value=False)\n",
    "            refiner_steps = gr.Slider(1, 50, value=10, label=\"Refiner Steps\")\n",
    "            lora_model = gr.Textbox(label=\"LoRA Model ID (HF repo)\", placeholder=\"ostris/super-realistic-xl\")\n",
    "            lora_scale = gr.Slider(0.0, 2.0, value=0.5, label=\"LoRA Scale\")\n",
    "            advanced.change(\n",
    "                fn=lambda t: gr.update(visible=t),\n",
    "                inputs=advanced,\n",
    "                outputs=[use_refiner, refiner_steps, lora_model, lora_scale],\n",
    "            )\n",
    "            btn = gr.Button(\"Generate Image\")\n",
    "        with gr.Column(scale=2):\n",
    "            output = gr.Image(label=\"Generated Image\", type=\"pil\")\n",
    "    btn.click(\n",
    "        fn=generate_image,\n",
    "        inputs=[prompt, negative_prompt, num_steps, guidance_scale, use_refiner, refiner_steps, lora_model, lora_scale, advanced],\n",
    "        outputs=output,\n",
    "    )\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SDXL_Colab_Stable",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
