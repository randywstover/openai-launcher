# ðŸŽ¨ Stable Diffusion XL Image Generator
# Generate high-quality images with Stable Diffusion XL (SDXL) using Colab's free GPU.
# No installation or API key required.

# ## Setup
# First, let's install the required packages:
# !pip install -q diffusers transformers accelerate gradio torch
# !pip install -q xformers==0.0.22 --index-url https://download.pytorch.org/whl/cu118

# ## Import Dependencies
import torch
from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline
import gradio as gr
from PIL import Image
import os # For potential future use, good to have

# ## Load Models

# --- Configuration ---
# Set to True if running on a system with limited VRAM to offload models to CPU when not in use.
# This will be slower but might prevent out-of-memory errors.
LOW_VRAM_MODE = False
# ---------------------


# Base model
# Initialize pipe as None. It will be loaded when first needed or explicitly.
pipe = None
refiner = None

def load_base_model():
    global pipe
    if pipe is None:
        print("Loading SDXL Base Model...")
        pipe = StableDiffusionXLPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16,
            variant="fp16",
            use_safetensors=True
        )
        pipe.to("cuda")
        # Enable memory efficient attention
        try:
            pipe.enable_xformers_memory_efficient_attention()
            print("xFormers memory efficient attention enabled for base model.")
        except ImportError:
            print("xFormers not available. Running without memory efficient attention for base model.")
        except Exception as e:
            print(f"Could not enable xFormers for base model: {e}")
        print("SDXL Base Model loaded.")
    elif LOW_VRAM_MODE: # If already loaded but in low VRAM mode, ensure it's on CUDA
        pipe.to("cuda")


def load_refiner_model():
    global refiner
    if refiner is None:
        print("Loading SDXL Refiner Model...")
        refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-refiner-1.0",
            torch_dtype=torch.float16,
            variant="fp16",
            use_safetensors=True
        )
        refiner.to("cuda")
        try:
            refiner.enable_xformers_memory_efficient_attention()
            print("xFormers memory efficient attention enabled for refiner model.")
        except ImportError:
            print("xFormers not available. Running without memory efficient attention for refiner model.")
        except Exception as e:
            print(f"Could not enable xFormers for refiner model: {e}")
        print("SDXL Refiner Model loaded.")
    elif LOW_VRAM_MODE: # If already loaded but in low VRAM mode, ensure it's on CUDA
        refiner.to("cuda")

# Pre-load base model on script start if not in LOW_VRAM_MODE
if not LOW_VRAM_MODE:
    load_base_model()

# ## Create Generation Function
def generate_image(prompt, negative_prompt="", num_steps=30, guidance_scale=7.5,
                   use_refiner_checkbox=False, refiner_steps=20, lora_model_id="", lora_scale_slider=0.8,
                   advanced_mode_checkbox=False, seed=-1):
    """
    Generates an image based on the provided parameters.
    Manages LoRA loading/unloading and refiner usage.
    """
    global pipe, refiner

    # Initialize LoRA state variables if they don't exist on the function object
    if not hasattr(generate_image, "active_lora_id"):
        generate_image.active_lora_id = None
    if not hasattr(generate_image, "active_lora_scale"):
        generate_image.active_lora_scale = None

    try:
        # Ensure base model is loaded and on CUDA
        load_base_model()

        # Determine the generator for reproducibility if seed is provided
        generator = torch.Generator(device="cuda")
        if seed == -1 or seed is None: # Gradio might pass None for empty number input
            generator.seed() # Random seed
        else:
            generator.manual_seed(int(seed))


        # --- LoRA Management ---
        lora_applied_this_run = False
        if advanced_mode_checkbox and lora_model_id.strip():
            requested_lora_id = lora_model_id.strip()
            requested_lora_scale = float(lora_scale_slider)

            # Check if LoRA needs to be changed (new ID or new scale)
            if (generate_image.active_lora_id != requested_lora_id or
                    generate_image.active_lora_scale != requested_lora_scale):

                print(f"Changing LoRA. Previous: {generate_image.active_lora_id} @ {generate_image.active_lora_scale}. Requested: {requested_lora_id} @ {requested_lora_scale}")

                # Unfuse and unload any previously active LoRA
                if generate_image.active_lora_id:
                    try:
                        print(f"Unfusing LoRA: {generate_image.active_lora_id}")
                        pipe.unfuse_lora()
                    except Exception as e:
                        print(f"Warning: Could not unfuse LoRA: {e}")
                    try:
                        print(f"Unloading all LoRA weights.")
                        pipe.unload_lora_weights() # Unloads all LoRAs
                    except Exception as e:
                        print(f"Warning: Could not unload LoRA weights: {e}")
                    generate_image.active_lora_id = None
                    generate_image.active_lora_scale = None

                # Load and fuse the new LoRA
                try:
                    print(f"Loading LoRA weights for: {requested_lora_id}")
                    pipe.load_lora_weights(requested_lora_id) # Loads into the UNet
                    print(f"Fusing LoRA {requested_lora_id} with scale {requested_lora_scale}")
                    pipe.fuse_lora(lora_scale=requested_lora_scale)
                    generate_image.active_lora_id = requested_lora_id
                    generate_image.active_lora_scale = requested_lora_scale
                    lora_applied_this_run = True
                except Exception as e:
                    print(f"Error loading or fusing LoRA model '{requested_lora_id}': {e}")
                    # Optionally, return an error image or raise a Gradio error
                    error_img = Image.new("RGB", (1024, 1024), color="black")
                    # from PIL import ImageDraw
                    # draw = ImageDraw.Draw(error_img)
                    # draw.text((10, 10), f"LoRA Error: {e}", fill="red")
                    # return error_img, "LoRA Error" # Returning a tuple for image and status
                    raise gr.Error(f"Failed to load LoRA: {requested_lora_id}. Check model ID and ensure it's compatible with SDXL. Error: {e}")

            else: # LoRA is already active and matches the request
                print(f"LoRA {generate_image.active_lora_id} already active with scale {generate_image.active_lora_scale}.")
                lora_applied_this_run = True # Considered applied if it's the one we want

        elif generate_image.active_lora_id: # Advanced mode off OR no LoRA ID, but one was active
            print(f"Advanced mode off or LoRA model cleared. Unfusing and unloading active LoRA: {generate_image.active_lora_id}")
            try:
                pipe.unfuse_lora()
            except Exception as e:
                print(f"Warning: Could not unfuse LoRA during cleanup: {e}")
            try:
                pipe.unload_lora_weights()
            except Exception as e:
                print(f"Warning: Could not unload LoRA weights during cleanup: {e}")
            generate_image.active_lora_id = None
            generate_image.active_lora_scale = None
        # --- End LoRA Management ---

        # Generate base image
        print(f"Generating base image with prompt: '{prompt}'")
        base_image = pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=int(num_steps),
            guidance_scale=float(guidance_scale),
            generator=generator,
            output_type="pil" # Ensure PIL image output for refiner
        ).images[0]
        print("Base image generated.")

        output_image = base_image

        # Apply refiner if enabled in advanced mode
        if advanced_mode_checkbox and use_refiner_checkbox:
            load_refiner_model() # Ensure refiner is loaded and on CUDA
            print(f"Applying refiner with prompt: '{prompt}'")
            output_image = refiner(
                prompt=prompt,
                negative_prompt=negative_prompt, # Refiner can also use negative_prompt
                num_inference_steps=int(refiner_steps),
                image=base_image, # Pass the base image
                generator=generator # Can reuse generator or create new
            ).images[0]
            print("Refiner applied.")
            if LOW_VRAM_MODE and refiner:
                print("Offloading refiner to CPU (Low VRAM Mode)")
                refiner.to("cpu")
                torch.cuda.empty_cache()


        # If a LoRA was applied for this specific run and then we are done with it,
        # and if we want to ensure it's not active for a *subsequent* non-LoRA call
        # *without* changing LoRA settings, we might unfuse here.
        # However, the current logic at the start of the function handles state changes,
        # so this explicit unfuse might be redundant unless strict immediate cleanup is desired.
        # For now, relying on the state check at the beginning of the next call.
        # if lora_applied_this_run and generate_image.active_lora_id:
        #     print(f"Unfusing LoRA {generate_image.active_lora_id} post-generation (optional step).")
        #     pipe.unfuse_lora()
            # Note: We don't unload weights here, as it might be used again with same ID.
            # The logic at the start handles unloading if the ID changes or LoRA is disabled.

        if LOW_VRAM_MODE and pipe:
            print("Offloading base model to CPU (Low VRAM Mode)")
            pipe.to("cpu")
            torch.cuda.empty_cache()

        return output_image, f"Seed: {generator.initial_seed()}"

    except Exception as e:
        print(f"Error generating image: {e}")
        import traceback
        traceback.print_exc()
        # Return black image and error message as fallback
        error_fallback_img = Image.new("RGB", (1024, 1024), color="black")
        # from PIL import ImageDraw
        # draw = ImageDraw.Draw(error_fallback_img)
        # draw.text((10, 10), f"Error: {e}", fill="red")
        # return error_fallback_img, f"Error: {e}"
        raise gr.Error(f"An error occurred during image generation: {e}")


# ## Create Gradio Interface
with gr.Blocks(css="body {font-family: sans-serif;}") as interface:
    gr.Markdown(
        """
        # ðŸŽ¨ Stable Diffusion XL Image Generator
        Generate high-quality images from text descriptions using SDXL 1.0.
        Supports advanced features like LoRA and a refiner model.
        """
    )

    with gr.Row():
        with gr.Column(scale=2):
            prompt_input = gr.Textbox(
                label="Prompt",
                placeholder="Enter your image description here... e.g., 'A majestic lion in the savanna at sunset, highly detailed, professional photography'",
                lines=3,
                elem_id="prompt_input"
            )
            negative_prompt_input = gr.Textbox(
                label="Negative Prompt",
                placeholder="What you don't want in the image... e.g., 'blurry, low quality, ugly, deformed, watermark'",
                lines=2,
                elem_id="negative_prompt_input"
            )

            with gr.Row():
                num_steps_slider = gr.Slider(
                    minimum=10, maximum=100, value=30, step=1,
                    label="Number of Inference Steps (Base Model)",
                    elem_id="num_steps_slider"
                )
                guidance_scale_slider = gr.Slider(
                    minimum=1.0, maximum=20.0, value=7.5, step=0.1,
                    label="Guidance Scale (Classifier-Free Guidance)",
                    elem_id="guidance_scale_slider"
                )

            seed_input = gr.Number(label="Seed (-1 for random)", value=-1, precision=0, elem_id="seed_input")

            advanced_mode_toggle = gr.Checkbox(label="Enable Advanced Mode (Refiner, LoRA)", value=False, elem_id="advanced_mode_toggle")

            with gr.Group(visible=False) as advanced_options_group:
                gr.Markdown("### Advanced Options")
                with gr.Tabs():
                    with gr.TabItem("Refiner"):
                        use_refiner_toggle = gr.Checkbox(label="Use SDXL Refiner Model", value=False, elem_id="use_refiner_toggle")
                        refiner_steps_slider = gr.Slider(
                            minimum=5, maximum=50, value=20, step=1,
                            label="Refiner Inference Steps",
                            elem_id="refiner_steps_slider"
                        )
                    with gr.TabItem("LoRA"):
                        lora_model_id_input = gr.Textbox(
                            label="LoRA Model ID (from Hugging Face)",
                            placeholder="e.g., 'ostris/super-realistic-xl' or 'minimaxir/sdxl-wrong-lora'",
                            elem_id="lora_model_id_input"
                        )
                        lora_scale_slider_input = gr.Slider(
                            minimum=0.0, maximum=2.0, value=0.8, step=0.05,
                            label="LoRA Scale (Weight)",
                            elem_id="lora_scale_slider_input"
                        )
                        gr.Markdown("<p style='font-size:0.9em; color:gray;'>Ensure the LoRA is compatible with SDXL 1.0. Some LoRAs might be for other base models (e.g., SD 1.5) and will cause errors.</p>")


            # Toggle visibility of advanced options
            advanced_mode_toggle.change(
                fn=lambda x: gr.update(visible=x),
                inputs=advanced_mode_toggle,
                outputs=advanced_options_group
            )

            generate_button = gr.Button("Generate Image", variant="primary", elem_id="generate_button")

        with gr.Column(scale=2):
            output_image_display = gr.Image(label="Generated Image", type="pil", height=768, elem_id="output_image_display")
            status_text_display = gr.Textbox(label="Status / Seed", interactive=False, elem_id="status_text_display")


    gr.Examples(
        examples=[
            ["A majestic lion in the savanna at sunset, highly detailed, professional photography", "blurry, low quality, cartoon", 30, 7.5, False, 20, "", 0.8, False, 12345],
            ["Epic portrait of a Warhammer 40k Space Marine, cinematic lighting, ultra realistic", "childish, cute, poorly drawn", 40, 8.0, True, 25, "", 0.8, True, 54321],
            ["A serene landscape of a mystical forest with glowing mushrooms and a hidden waterfall, fantasy art", "dark, scary, ugly", 35, 7.0, False, 20, "", 0.8, False, 98765],
            ["Photorealistic image of an astronaut discovering an ancient alien artifact on Mars, detailed suit, dramatic lighting", "cartoon, flat, unrealistic", 50, 9.0, True, 30, "ostris/super-realistic-xl", 0.9, True, 11223]
        ],
        inputs=[prompt_input, negative_prompt_input, num_steps_slider, guidance_scale_slider,
                use_refiner_toggle, refiner_steps_slider, lora_model_id_input, lora_scale_slider_input,
                advanced_mode_toggle, seed_input],
        outputs=[output_image_display, status_text_display],
        fn=generate_image, # Ensure this function can handle the tuple output
        cache_examples=False # Set to True if you want to cache example results
    )

    # Connect button to function
    generate_button.click(
        fn=generate_image,
        inputs=[prompt_input, negative_prompt_input, num_steps_slider, guidance_scale_slider,
                use_refiner_toggle, refiner_steps_slider, lora_model_id_input, lora_scale_slider_input,
                advanced_mode_toggle, seed_input],
        outputs=[output_image_display, status_text_display]
    )

# To run this:
# 1. Ensure you have a GPU runtime in Colab or a local setup with CUDA.
# 2. Uncomment the !pip install lines if you haven't installed the packages.
# 3. Execute all cells.
if __name__ == "__main__":
    # Optional: Pre-load models if not already done by the global scope call
    # (though load_base_model() is called if not LOW_VRAM_MODE already)
    # if pipe is None and not LOW_VRAM_MODE:
    #    load_base_model()

    print("Launching Gradio Interface...")
    interface.launch(share=True, debug=True) # share=True for public link, debug=True for more logs
