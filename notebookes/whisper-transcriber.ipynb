{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤ Whisper Speech-to-Text Transcriber\n",
    "\n",
    "This notebook uses OpenAI's Whisper model to transcribe audio files into text. It runs entirely in Colab's free tier and requires no API key.\n",
    "\n",
    "## Model Information\n",
    "Whisper is a state-of-the-art speech recognition model that offers:\n",
    "\n",
    "- **Multilingual Support**: Transcribe audio in 99 languages\n",
    "- **Multiple Model Sizes**:\n",
    "  - `tiny` (39M params): Ultra-fast, good for quick tests\n",
    "  - `base` (74M params): Good balance of speed and accuracy\n",
    "  - `small` (244M params): Better accuracy, still fast\n",
    "  - `medium` (769M params): High accuracy\n",
    "  - `large-v3` (1.5B params): Best quality, latest version\n",
    "  - `turbo` (Optimized): Fast English transcription\n",
    "\n",
    "## Features\n",
    "- Automatic language detection\n",
    "- Support for multiple audio formats (mp3, wav, m4a, etc.)\n",
    "- Fast transcription using GPU acceleration\n",
    "- Optional timestamp generation\n",
    "- Advanced features:\n",
    "  - Word-level timestamps\n",
    "  - Speaker diarization\n",
    "  - Confidence scores\n",
    "  - Custom vocabulary\n",
    "\n",
    "## Setup\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/openai/whisper.git\n",
    "!pip install -q gradio pyannote.audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import gradio as gr\n",
    "import torch\n",
    "import os\n",
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available models and their descriptions\n",
    "MODELS = {\n",
    "    \"base\": \"Good balance of speed and accuracy\",\n",
    "    \"tiny\": \"Ultra-fast, good for quick tests\",\n",
    "    \"small\": \"Better accuracy, still fast\",\n",
    "    \"medium\": \"High accuracy\",\n",
    "    \"large-v3\": \"Best quality, latest version\",\n",
    "    \"turbo\": \"Optimized for English\"\n",
    "}\n",
    "\n",
    "def load_model(model_name=\"base\"):\n",
    "    model = whisper.load_model(model_name)\n",
    "    print(f\"Using {model_name} model on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    return model\n",
    "\n",
    "# Initialize with default model\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transcription Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path, model_size=\"base\", add_timestamps=False,\n",
    "                    word_timestamps=False, detect_speakers=False,\n",
    "                    language=None, task=\"transcribe\"):\n",
    "    try:\n",
    "        # Load selected model if different from current\n",
    "        global model\n",
    "        if model_size != model.model_size:\n",
    "            model = load_model(model_size)\n",
    "        \n",
    "        # Transcription options\n",
    "        options = {\n",
    "            \"task\": task,  # transcribe or translate\n",
    "            \"language\": language,  # None for auto-detection\n",
    "            \"word_timestamps\": word_timestamps\n",
    "        }\n",
    "        \n",
    "        # Transcribe audio\n",
    "        result = model.transcribe(audio_path, **options)\n",
    "        \n",
    "        # Initialize output text\n",
    "        output = \"\"\n",
    "        \n",
    "        # Add detected language if auto-detected\n",
    "        if not language:\n",
    "            output += f\"Detected Language: {result['language']}\\n\\n\"\n",
    "        \n",
    "        # Process with speaker diarization if requested\n",
    "        if detect_speakers:\n",
    "            pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n",
    "            diarization = pipeline(audio_path)\n",
    "            \n",
    "            # Merge diarization with transcription\n",
    "            for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
    "                start_time = segment.start\n",
    "                # Find corresponding text segments\n",
    "                for seg in result[\"segments\"]:\n",
    "                    if seg[\"start\"] >= start_time and seg[\"end\"] <= segment.end:\n",
    "                        output += f\"[{speaker}] {seg['text']}\\n\"\n",
    "        \n",
    "        # Format with timestamps if requested\n",
    "        elif add_timestamps:\n",
    "            if word_timestamps:\n",
    "                # Add word-level timestamps\n",
    "                for segment in result[\"segments\"]:\n",
    "                    for word in segment[\"words\"]:\n",
    "                        start = int(word[\"start\"])\n",
    "                        output += f\"[{start//60:02d}:{start%60:02d}.{int((start%1)*100):02d}] {word['text']} \"\n",
    "                    output += \"\\n\"\n",
    "            else:\n",
    "                # Add segment-level timestamps\n",
    "                for segment in result[\"segments\"]:\n",
    "                    start = int(segment[\"start\"])\n",
    "                    output += f\"[{start//60:02d}:{start%60:02d}] {segment['text']}\\n\"\n",
    "        else:\n",
    "            output += result[\"text\"]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error during transcription: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ğŸ¤ Whisper Speech-to-Text Transcriber\n",
    "        Upload an audio file to transcribe it using OpenAI's Whisper model.\n",
    "        \n",
    "        ### Tips for best results:\n",
    "        1. Use clear audio with minimal background noise\n",
    "        2. Choose larger models for better accuracy\n",
    "        3. Enable timestamps for long recordings\n",
    "        4. Use speaker detection for multi-speaker audio\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            audio_input = gr.Audio(\n",
    "                type=\"filepath\",\n",
    "                label=\"Upload Audio\"\n",
    "            )\n",
    "            \n",
    "            model_choice = gr.Dropdown(\n",
    "                choices=list(MODELS.keys()),\n",
    "                value=\"base\",\n",
    "                label=\"Model Size\",\n",
    "                info=\"Larger models are more accurate but slower\"\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                task = gr.Radio(\n",
    "                    choices=[\"transcribe\", \"translate\"],\n",
    "                    value=\"transcribe\",\n",
    "                    label=\"Task\",\n",
    "                    info=\"Translate will convert to English\"\n",
    "                )\n",
    "                language = gr.Dropdown(\n",
    "                    choices=[None] + sorted(whisper.tokenizer.LANGUAGES.keys()),\n",
    "                    value=None,\n",
    "                    label=\"Language\",\n",
    "                    info=\"Auto-detect if not specified\"\n",
    "                )\n",
    "            \n",
    "            with gr.Row():\n",
    "                timestamps = gr.Checkbox(\n",
    "                    label=\"Add timestamps\",\n",
    "                    info=\"Add time markers to the transcript\"\n",
    "                )\n",
    "                word_level = gr.Checkbox(\n",
    "                    label=\"Word-level timestamps\",\n",
    "                    info=\"Add timestamps for each word\"\n",
    "                )\n",
    "                speakers = gr.Checkbox(\n",
    "                    label=\"Detect speakers\",\n",
    "                    info=\"Identify different speakers in the audio\"\n",
    "                )\n",
    "        \n",
    "        with gr.Column(scale=2):\n",
    "            output_text = gr.Textbox(\n",
    "                label=\"Transcription\",\n",
    "                lines=20\n",
    "            )\n",
    "    \n",
    "    gr.Examples([\n",
    "        [\"sample.mp3\", \"base\", True, False, False, None, \"transcribe\"],\n",
    "        [\"sample.mp3\", \"large-v3\", True, True, False, \"en\", \"transcribe\"],\n",
    "        [\"sample.mp3\", \"medium\", False, False, True, None, \"translate\"]\n",
    "    ], [audio_input, model_choice, timestamps, word_level, speakers, language, task])\n",
    "    \n",
    "    inputs = [audio_input, model_choice, timestamps, word_level, speakers, language, task]\n",
    "    \n",
    "    gr.Interface(fn=transcribe_audio, inputs=inputs, outputs=output_text)\n",
    "\n",
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Whisper Transcriber",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
